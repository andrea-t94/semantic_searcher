{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"trec-product-search/Product-Search-Triples\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query_id', 'query', 'positive_passages', 'negative_passages'],\n",
       "    num_rows: 20888\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is made such that, for each text query we have multiple positive and negative passages.\n",
    "\n",
    "Considering that, on average, we have 20 positive passages per anchor and 100 negatives, this means we have 20888*20 = 417760 (anchor, positive) pairs. Each of this pair will have approx 100 negative pairs to be trained with --> **4M rows in total**. \n",
    "\n",
    "For the assigment purposes, is not possible to train on the whole dataset, therefore I decided to down-sample it.\n",
    "\n",
    "**How do we downsample it for training?**\n",
    "- I take only **one positive and negative passage per query** - the idea behidn is to at least guarantee 100% coverage for all the queries \n",
    "- If still doesn't work I will randomly downsample the remaining dataet (spoiler: I will have to do it)\n",
    "\n",
    "**Which loss function is suited for the dataset?**  \n",
    "Considering the simplified setup, both MultipleNegativesRankingLoss and TripletLoss would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 18.77245308310992\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for passages in train_dataset['positive_passages']:\n",
    "    for passage in passages:\n",
    "        cnt += 1\n",
    "print(\"train: \" + str(cnt/len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 98.42378399080812\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for passages in train_dataset['negative_passages']:\n",
    "    for passage in passages:\n",
    "        cnt += 1\n",
    "print(\"train: \" + str(cnt/len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating query lenght against positive and negative passages\n",
    "\n",
    "There are three main insights about this:\n",
    "- in general texts are short and none that exceeded token limit of 512\n",
    "- query lenght is shorter than title lenght\n",
    "- test dataset do not contain any passage\n",
    "\n",
    "Given that:\n",
    "- I choose a max_len that is 99th percentile to optimise model training and inference (since I'm working on local cpu)\n",
    "- I choose pre-trained models specialised in asymmetric semantic search retrieval (more info https://sbert.net/examples/applications/semantic-search/README.html#examples)\n",
    "- For model evaluation I will split dev dataset into two, one for validation and one for testing against the baseline\n",
    "\n",
    "**What about texts longer than 99th percentile??**  \n",
    "I am going to truncate the remaining aprt of the text, since 99th percentile is way lower the max model lenght.  \n",
    "\n",
    "I am not going to consider remaining chunks because it would over complicate things since the tokenizer would create n>1 chunks for each query:\n",
    "- I would need to decide how to treat them: for example I might want to add some stride among chunks\n",
    "- I would define some data processing to give each chunk same query\n",
    "- I would consider custom training: for example a custom loss which is the mean across the  chunks because \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing accordingly to what I've said above to simplify the calculations\n",
    "def process_dataset(dataset):\n",
    "    def process_triplet(example):\n",
    "        has_valid_data = (\n",
    "            isinstance(example['positive_passages'], list) and \n",
    "            isinstance(example['negative_passages'], list) and \n",
    "            len(example['positive_passages']) > 0 and \n",
    "            len(example['negative_passages']) > 0\n",
    "        )\n",
    "        \n",
    "        if has_valid_data:\n",
    "            processed =  {\n",
    "                'query': example['query'],\n",
    "                'positive_passages': example['positive_passages'][0]['title'],\n",
    "                'negative_passages': example['negative_passages'][0]['title']\n",
    "            }\n",
    "        else:\n",
    "            # Return a placeholder with is_valid=False instead of None\n",
    "            processed = {\n",
    "                'query': '',  # Empty placeholder\n",
    "                'positive_passages': '',\n",
    "                'negative_passages': ''\n",
    "            }\n",
    "    \n",
    "        # Add a validation flag\n",
    "        processed['valid'] = bool(has_valid_data)\n",
    "        return processed\n",
    "    \n",
    "    # Apply the processing function to the dataset\n",
    "    processed_dataset = dataset.map(process_triplet)\n",
    "    # Then filter to keep only valid examples\n",
    "    final_dataset = processed_dataset.filter(lambda x: x['valid'])\n",
    "    \n",
    "    # Remove the validation flag since we don't need it anymore\n",
    "    final_dataset = final_dataset.remove_columns(['query_id','valid'])\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = process_dataset(load_dataset(\"trec-product-search/Product-Search-Triples\", split=\"train\"))\n",
    "eval_dataset = process_dataset(load_dataset(\"trec-product-search/Product-Search-Triples\", split=\"dev\"))\n",
    "test_dataset = process_dataset(load_dataset(\"trec-product-search/Product-Search-Triples\", split=\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor texts - Mean: 7.10, 99th: 12.67, Max: 38.00, Min: 3.00\n",
      "Positive texts - Mean: 28.79, 99th: 62.13, Max: 109.00, Min: 2.00\n",
      "Negative texts - Mean: 28.98, 99th: 61.44, Max: 322.00, Min: 2.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/msmarco-MiniLM-L-6-v3')\n",
    "# Function to calculate lengths\n",
    "def get_length_stats(texts):\n",
    "    lengths = [len(tokenizer.encode(text)) for text in texts]\n",
    "    return {\n",
    "        'mean': np.mean(lengths),\n",
    "        'std': np.std(lengths),\n",
    "        'min': np.min(lengths),\n",
    "        'max': np.max(lengths),\n",
    "        'distribution': lengths\n",
    "    }\n",
    "\n",
    "anchor_stats = get_length_stats(train_dataset['query'])\n",
    "positive_stats = get_length_stats(train_dataset['positive_passages'])\n",
    "negative_stats = get_length_stats(train_dataset['negative_passages'])\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Anchor texts - Mean: {anchor_stats['mean']:.2f}, 99th: {anchor_stats['mean']+anchor_stats['std']*2.3:.2f}, Max: {anchor_stats['max']:.2f}, Min: {anchor_stats['min']:.2f}\")\n",
    "print(f\"Positive texts - Mean: {positive_stats['mean']:.2f}, 99th: {positive_stats['mean']+positive_stats['std']*2.3:.2f}, Max: {positive_stats['max']:.2f}, Min: {positive_stats['min']:.2f}\")\n",
    "print(f\"Negative texts - Mean: {negative_stats['mean']:.2f}, 99th: {negative_stats['mean']+ negative_stats['std']*2.3:.2f}, Max: {negative_stats['max']:.2f}, Min: {negative_stats['min']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor texts - Mean: 7.09, 99th: 12.60, Max: 36.00, Min: 3.00\n",
      "Positive texts - Mean: 28.68, 99th: 61.84, Max: 117.00, Min: 2.00\n",
      "Negative texts - Mean: 28.87, 99th: 61.01, Max: 172.00, Min: 2.00\n"
     ]
    }
   ],
   "source": [
    "anchor_stats = get_length_stats(eval_dataset['query'])\n",
    "positive_stats = get_length_stats(eval_dataset['positive_passages'])\n",
    "negative_stats = get_length_stats(eval_dataset['negative_passages'])\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Anchor texts - Mean: {anchor_stats['mean']:.2f}, 99th: {anchor_stats['mean']+anchor_stats['std']*2.3:.2f}, Max: {anchor_stats['max']:.2f}, Min: {anchor_stats['min']:.2f}\")\n",
    "print(f\"Positive texts - Mean: {positive_stats['mean']:.2f}, 99th: {positive_stats['mean']+positive_stats['std']*2.3:.2f}, Max: {positive_stats['max']:.2f}, Min: {positive_stats['min']:.2f}\")\n",
    "print(f\"Negative texts - Mean: {negative_stats['mean']:.2f}, 99th: {negative_stats['mean']+ negative_stats['std']*2.3:.2f}, Max: {negative_stats['max']:.2f}, Min: {negative_stats['min']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'positive_passages', 'negative_passages'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate if there are too many out of bag words for our model\n",
    "\n",
    "There are no infrequent words we need to deal with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis for Query texts:\n",
      "Total tokens: 105997\n",
      "Unknown tokens: 0 (0.00%)\n",
      "\n",
      "Analysis for Positive texts:\n",
      "Total tokens: 557099\n",
      "Unknown tokens: 121 (0.02%)\n",
      "\n",
      "Analysis for Negative texts:\n",
      "Total tokens: 560969\n",
      "Unknown tokens: 64 (0.01%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def analyze_unknown_tokens(texts, text_type):\n",
    "    \"\"\"\n",
    "    Analyzes the presence of unknown tokens in a collection of texts.\n",
    "    We'll track both the frequency of unknown tokens and which original words tend to cause them.\n",
    "    \"\"\"\n",
    "    # Counter for unknown tokens\n",
    "    unknown_token_count = 0\n",
    "    total_token_count = 0\n",
    "    \n",
    "    # Track which words often lead to unknown tokens\n",
    "    problematic_words = Counter()\n",
    "    \n",
    "    # The unknown token ID for this tokenizer\n",
    "    unknown_token_id = tokenizer.unk_token_id\n",
    "    \n",
    "    for text in texts:\n",
    "        # Get both tokens and their IDs\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        words = text.split()\n",
    "        \n",
    "        # If we find unknown tokens, let's see which words might have caused them\n",
    "        if unknown_token_id in tokens:\n",
    "            # Tokenize each word separately to find problematic ones\n",
    "            for word in words:\n",
    "                word_tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "                if unknown_token_id in word_tokens:\n",
    "                    problematic_words[word] += 1\n",
    "        \n",
    "        # Count unknowns in this text\n",
    "        unknown_token_count += tokens.count(unknown_token_id)\n",
    "        total_token_count += len(tokens)\n",
    "    \n",
    "    # Calculate percentage\n",
    "    unknown_percentage = (unknown_token_count / total_token_count * 100) if total_token_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nAnalysis for {text_type}:\")\n",
    "    print(f\"Total tokens: {total_token_count}\")\n",
    "    print(f\"Unknown tokens: {unknown_token_count} ({unknown_percentage:.2f}%)\")\n",
    "    \n",
    "    \n",
    "    return unknown_percentage, problematic_words\n",
    "\n",
    "# Analyze query texts\n",
    "query_unknown = analyze_unknown_tokens(train_dataset['query'], 'Query texts')\n",
    "\n",
    "# Analyze positive texts\n",
    "positive_unknown = analyze_unknown_tokens(train_dataset['positive_passages'], 'Positive texts')\n",
    "\n",
    "# Analyze negative texts\n",
    "negative_unknown = analyze_unknown_tokens(train_dataset['negative_passages'], 'Negative texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test dataset for IR evaluator. \n",
    "\n",
    "For evaluation purposes I prefer to use the **InformationRetrievalEvaluator** to test model performances on semantic search (which is the final goal of the fine tuning).   \n",
    "For training I will instead use the **TripletEvaluator** which only measure the cosine similarity accuracy.\n",
    "To do so I need to have a list of relevant passages for each query: I will use the positive passages from each query.  \n",
    "\n",
    "Test dataset will be split into:\n",
    "- queries: Dict[str] containing all the test query_id:text values\n",
    "- relevant_docs: Dict[List[str]], containing for each query_id, the positive passages docid\n",
    "- corpus: Dict[str], containing mapping docid: text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dev dataset into validation dataset for training, and test dataset for model evaluation\n",
    "eval_dataset_raw = load_dataset(\"trec-product-search/Product-Search-Triples\", split=\"train\")\n",
    "split_dataset = eval_dataset_raw.train_test_split(test_size=0.05)\n",
    "eval_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def process_test_dataset(dataset):\n",
    "    \n",
    "    # Initialize our output dictionaries\n",
    "    queries: Dict[str, str] = {}\n",
    "    relevant_docs: Dict[str, List[str]] = defaultdict(list)\n",
    "    corpus: Dict[str, str] = {}\n",
    "    \n",
    "    # Process each split in the dataset\n",
    "    for sample in dataset:\n",
    "        query_id = sample['query_id']\n",
    "        query = sample['query']\n",
    "        positives = sample['positive_passages']\n",
    "        negatives = sample['negative_passages']\n",
    "        \n",
    "        # Store query\n",
    "        queries[query_id] = query\n",
    "        \n",
    "        # Process positive documents\n",
    "        for pos_doc in positives:\n",
    "            doc_id = pos_doc['docid']\n",
    "            title = pos_doc['title']\n",
    "            relevant_docs[query_id].append(doc_id)\n",
    "            corpus[doc_id] = title\n",
    "            \n",
    "        # Process negative documents (for corpus only)\n",
    "        for neg_doc in negatives:\n",
    "            doc_id = neg_doc['docid']\n",
    "            title = neg_doc['title']\n",
    "            corpus[doc_id] = title\n",
    "    \n",
    "    return queries, relevant_docs, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, relevant_docs, corpus = process_test_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "\n",
    "# Given queries, a corpus and a mapping with relevant documents, the InformationRetrievalEvaluator computes different IR metrics.\n",
    "model = SentenceTransformer('sentence-transformers/msmarco-MiniLM-L-6-v3')\n",
    "ir_evaluator = InformationRetrievalEvaluator(\n",
    "    queries=queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs,\n",
    "    name=\"trec-dataset-test\",\n",
    ")\n",
    "results = ir_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trec-dataset-test_cosine_accuracy@1': 0.3946360153256705,\n",
       " 'trec-dataset-test_cosine_accuracy@3': 0.5670498084291188,\n",
       " 'trec-dataset-test_cosine_accuracy@5': 0.6388888888888888,\n",
       " 'trec-dataset-test_cosine_accuracy@10': 0.7375478927203065,\n",
       " 'trec-dataset-test_cosine_precision@1': 0.3946360153256705,\n",
       " 'trec-dataset-test_cosine_precision@3': 0.33876117496807157,\n",
       " 'trec-dataset-test_cosine_precision@5': 0.3065134099616858,\n",
       " 'trec-dataset-test_cosine_precision@10': 0.2522030651340996,\n",
       " 'trec-dataset-test_cosine_recall@1': 0.023488977943643224,\n",
       " 'trec-dataset-test_cosine_recall@3': 0.059354064751735096,\n",
       " 'trec-dataset-test_cosine_recall@5': 0.08781059630679747,\n",
       " 'trec-dataset-test_cosine_recall@10': 0.14169269594896383,\n",
       " 'trec-dataset-test_cosine_ndcg@10': 0.282676791488999,\n",
       " 'trec-dataset-test_cosine_mrr@10': 0.5003679377242592,\n",
       " 'trec-dataset-test_cosine_map@100': 0.1683177674005804}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
